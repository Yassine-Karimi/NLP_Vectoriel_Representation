{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fc8c24a",
   "metadata": {},
   "source": [
    "# Atelier1 : Représentation vectorielle de Texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6df0012",
   "metadata": {},
   "source": [
    "### Partie 1 : Pré-traitement de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ccbff2",
   "metadata": {},
   "source": [
    "1. Créer un corpus qui contient les textes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "645cff32",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fbde45",
   "metadata": {},
   "source": [
    "2. Convertir le corpus en type DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c67835c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte\n",
      "0        Le chat dort sur le tapis.\n",
      "1    Les Oiseaux Chantent Le Matin.\n",
      "2    Le chien court dans le jardin.\n",
      "3  Mangeons des pommes délicieuses.\n",
      "4      Je mange une orange fraîche.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'texte': corpus})\n",
    "\n",
    "# Affichez le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45c1ed",
   "metadata": {},
   "source": [
    "3. Quelles sont les différentes approches pour prétraiter un corpus de textes? Décrire les.\n",
    "C’est quoi le rôle de chaque approche ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e08b5",
   "metadata": {},
   "source": [
    "## Tokenization (Tokenisation) :\n",
    "\n",
    "* Rôle :\n",
    "La tokenization consiste à diviser un texte en unités discrètes, généralement en mots ou en tokens. Cela permet de segmenter le texte en éléments distincts pour une analyse plus approfondie. Les tokens sont essentiels pour effectuer des opérations telles que la comptage de mots, l'analyse grammaticale, et la recherche d'entités nommées.\n",
    "\n",
    "## Suppression de la ponctuation :\n",
    "\n",
    "* Rôle : La suppression de la ponctuation implique de retirer les caractères de ponctuation du texte, tels que les points, les virgules, les guillemets, etc. Cela permet de nettoyer le texte des éléments non alphabétiques qui n'apportent généralement pas d'informations significatives pour de nombreuses tâches d'analyse de texte.\n",
    "\n",
    "## Mise en minuscules :\n",
    "\n",
    "* Rôle :\n",
    "La mise en minuscules consiste à convertir l'ensemble du texte en lettres minuscules. Cela permet de normaliser le texte et d'éviter les variations dues à la casse. Par exemple, \"Chat\" et \"chat\" seront considérés comme identiques après mise en minuscules.\n",
    "\n",
    "## Suppression des stopwords :\n",
    "\n",
    "* Rôle : \n",
    "Les stopwords sont des mots courants tels que \"le\", \"et\", \"de\" qui n'apportent généralement pas d'informations significatives dans l'analyse de texte. Les supprimer du texte permet de réduire le bruit et de se concentrer sur les mots clés importants.\n",
    "\n",
    "## Lemmatization (Lemmatisation) :\n",
    "\n",
    "* Rôle :\n",
    "La lemmatisation vise à réduire les mots à leur forme de base, appelée lemme. Par exemple, \"mangeons\" serait réduit à \"manger\". Cela aide à regrouper différentes formes d'un mot pour une analyse plus cohérente.\n",
    "Stemming (Racinisation) :\n",
    "\n",
    "## Stemming (Racinisation) :\n",
    "\n",
    "* Rôle :\n",
    "Le stemming réduit les mots à leur racine ou à leur forme tronquée, même si la forme résultante n'est pas toujours un mot réel. Par exemple, \"mangeons\" pourrait être réduit à \"mang\". Cette technique est plus agressive que la lemmatisation mais permet de regrouper des mots similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972b57f",
   "metadata": {},
   "source": [
    "4. Décrire le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "06db4fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "def code(texte):\n",
    "    text_s_pon = [c for c in texte if c not in string.punctuation]\n",
    "    return text_s_pon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2396e28f",
   "metadata": {},
   "source": [
    "Ce code importe la bibliothèque string et imprime la ponctuation.\n",
    "Ensuite, il définit une fonction code(texte) qui prend un texte en entrée et renvoie le même\n",
    "texte, mais avec la ponctuation supprimée. \n",
    "La liste de caractères de ponctuation est obtenue à partir de string.punctuation. \n",
    "Par exemple, si vous appelez code(\"Le chat dort.\"), cela renverra \"Le chat dort\" en supprimant le point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f725e92d",
   "metadata": {},
   "source": [
    "5. Ajouter une colonne dans l’objet corpus nommée « t_s_p » en utilisant la fonction de Q.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7e8c4766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis\n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin\n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin\n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses\n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Créez un DataFrame à partir du corpus\n",
    "corpus = pd.DataFrame({'texte': [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]})\n",
    "\n",
    "# Fonction de suppression de la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    # Utilisez la fonction translate pour supprimer la ponctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punct = text.translate(translator)\n",
    "    return text_no_punct\n",
    "\n",
    "# Appliquez la fonction de suppression de la ponctuation à la colonne \"texte\"\n",
    "corpus['t_s_p'] = corpus['texte'].apply(remove_punctuation)\n",
    "\n",
    "# Affichez le DataFrame avec la nouvelle colonne\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb21d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed82951b",
   "metadata": {},
   "source": [
    "6. Ecrire une fonction pour tokenizer le corpus de colonne \"t_s_p\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9f06f927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens  \n",
      "0      [Le, chat, dort, sur, le, tapis]  \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]  \n",
      "2  [Le, chien, court, dans, le, jardin]  \n",
      "3  [Mangeons, des, pommes, délicieuses]  \n",
      "4     [Je, mange, une, orange, fraîche]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour la tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Appliquez la fonction de tokenization à la colonne \"t_s_p\"\n",
    "corpus['tokens'] = corpus['t_s_p'].apply(tokenize_text)\n",
    "\n",
    "# Affichez le DataFrame avec la nouvelle colonne \"tokens\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed36dd7",
   "metadata": {},
   "source": [
    "7. Ecrire une fonction qui élimine les stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6550f7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                [chat, dort, tapis]\n",
      "1         [Oiseaux, Chantent, Matin]\n",
      "2             [chien, court, jardin]\n",
      "3    [Mangeons, pommes, délicieuses]\n",
      "4           [mange, orange, fraîche]\n",
      "Name: no_stopwords, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé et téléchargez les stopwords si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Fonction pour éliminer les stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    # Utilisez les stopwords de la langue française\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in french_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "corpus['no_stopwords'] = corpus['tokens'].apply(remove_stopwords)\n",
    "print(corpus['no_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36dae9e",
   "metadata": {},
   "source": [
    "8. Appliquer la lemmatisation et stremming sur le corpus sans stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "4b86b601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens                     no_stopwords  \\\n",
      "0      [Le, chat, dort, sur, le, tapis]              [chat, dort, tapis]   \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]       [Oiseaux, Chantent, Matin]   \n",
      "2  [Le, chien, court, dans, le, jardin]           [chien, court, jardin]   \n",
      "3  [Mangeons, des, pommes, délicieuses]  [Mangeons, pommes, délicieuses]   \n",
      "4     [Je, mange, une, orange, fraîche]         [mange, orange, fraîche]   \n",
      "\n",
      "                        lemmatized                    stemmed  \n",
      "0              [chat, dort, tapis]        [chat, dort, tapis]  \n",
      "1       [Oiseaux, Chantent, Matin]  [oiseau, chantent, matin]  \n",
      "2           [chien, court, jardin]     [chien, court, jardin]  \n",
      "3  [Mangeons, pommes, délicieuses]    [mangeon, pomm, délici]  \n",
      "4         [mange, orange, fraîche]      [mang, orang, fraîch]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé et téléchargez les données nécessaires\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Fonction pour éliminer les stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in french_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Fonction pour lemmatiser les tokens\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "# Fonction pour appliquer le stemming aux tokens\n",
    "def stem(tokens):\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "\n",
    "# Appliquez la suppression des stopwords à la colonne \"t_s_p\"\n",
    "corpus['no_stopwords'] = corpus['t_s_p'].apply(nltk.word_tokenize).apply(remove_stopwords)\n",
    "\n",
    "# Appliquez la lemmatisation aux tokens sans stopwords\n",
    "corpus['lemmatized'] = corpus['no_stopwords'].apply(lemmatize)\n",
    "\n",
    "# Appliquez le stemming aux tokens lemmatisés\n",
    "corpus['stemmed'] = corpus['lemmatized'].apply(stem)\n",
    "\n",
    "# Affichez le DataFrame avec les nouvelles colonnes\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5561f1fa",
   "metadata": {},
   "source": [
    "# Partie 2 : CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3a50f",
   "metadata": {},
   "source": [
    "\n",
    "9. Initialiser et ajuster le CountVectorizer à votre corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "da28664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Créez une instance de CountVectorizer\n",
    "french_stopwords = [\"le\", \"la\", \"les\", \"de\", \"du\", \"des\", \"et\", \"en\", \"je\", \"tu\", \"il\", \"elle\", \"nous\", \"vous\", \"ils\", \"elles\", \"ce\", \"cette\", \"ces\"]\n",
    "\n",
    "# Créez une instance de CountVectorizer en spécifiant les stopwords\n",
    "count_vectorizer = CountVectorizer(stop_words=french_stopwords)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fce7b",
   "metadata": {},
   "source": [
    "10. Transformer le corpus en une matrice de comptage de tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "065d0ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 8)\t1\n",
      "  (3, 10)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 6)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 17)\t1\n",
      "  (4, 13)\t1\n",
      "  (4, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# X est la matrice de comptage des tokens\n",
    "\n",
    "X = count_vectorizer.fit_transform(corpus['t_s_p'])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72730a90",
   "metadata": {},
   "source": [
    "11. Explorer la matrice résultante pour comprendre comment les tokens sont représentés envecteurs binaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c5bda94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chantent</th>\n",
       "      <th>chat</th>\n",
       "      <th>chien</th>\n",
       "      <th>court</th>\n",
       "      <th>dans</th>\n",
       "      <th>dort</th>\n",
       "      <th>délicieuses</th>\n",
       "      <th>fraîche</th>\n",
       "      <th>jardin</th>\n",
       "      <th>mange</th>\n",
       "      <th>mangeons</th>\n",
       "      <th>matin</th>\n",
       "      <th>oiseaux</th>\n",
       "      <th>orange</th>\n",
       "      <th>pommes</th>\n",
       "      <th>sur</th>\n",
       "      <th>tapis</th>\n",
       "      <th>une</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chantent  chat  chien  court  dans  dort  délicieuses  fraîche  jardin  \\\n",
       "0         0     1      0      0     0     1            0        0       0   \n",
       "1         1     0      0      0     0     0            0        0       0   \n",
       "2         0     0      1      1     1     0            0        0       1   \n",
       "3         0     0      0      0     0     0            1        0       0   \n",
       "4         0     0      0      0     0     0            0        1       0   \n",
       "\n",
       "   mange  mangeons  matin  oiseaux  orange  pommes  sur  tapis  une  \n",
       "0      0         0      0        0       0       0    1      1    0  \n",
       "1      0         0      1        1       0       0    0      0    0  \n",
       "2      0         0      0        0       0       0    0      0    0  \n",
       "3      0         1      0        0       0       1    0      0    0  \n",
       "4      1         0      0        0       1       0    0      0    1  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X.toarray(), columns=count_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c9f71",
   "metadata": {},
   "source": [
    "# Partie 3 : TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812ea1aa",
   "metadata": {},
   "source": [
    "1. Initialiser et ajuster le TfidfVectorizer à votre corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d8eaa071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words\n",
    "# Créez une instance de TfidfVectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=french_stopwords, tokenizer=stemming_tokenizer, use_idf=False, norm='l1')\n",
    "\n",
    "\n",
    "porter_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28d7ec",
   "metadata": {},
   "source": [
    "2. Transformer le corpus en une matrice de poids de tokens dans le corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "19120b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hisoka\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\Hisoka\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['cett', 'ell', 'nou', 'vou'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_tfidf = tfidf_vectorizer.fit_transform(corpus['t_s_p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c900c85",
   "metadata": {},
   "source": [
    "3. Explorer la matrice résultante pour comprendre comment les tokens sont représentés.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e9c2684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chantent</th>\n",
       "      <th>chat</th>\n",
       "      <th>che</th>\n",
       "      <th>chien</th>\n",
       "      <th>court</th>\n",
       "      <th>d</th>\n",
       "      <th>dan</th>\n",
       "      <th>dort</th>\n",
       "      <th>fra</th>\n",
       "      <th>jardin</th>\n",
       "      <th>licieus</th>\n",
       "      <th>mang</th>\n",
       "      <th>mangeon</th>\n",
       "      <th>matin</th>\n",
       "      <th>oiseaux</th>\n",
       "      <th>orang</th>\n",
       "      <th>pomm</th>\n",
       "      <th>sur</th>\n",
       "      <th>tapi</th>\n",
       "      <th>une</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chantent  chat  che  chien  court     d   dan  dort  fra  jardin  licieus  \\\n",
       "0  0.000000  0.25  0.0   0.00   0.00  0.00  0.00  0.25  0.0    0.00     0.00   \n",
       "1  0.333333  0.00  0.0   0.00   0.00  0.00  0.00  0.00  0.0    0.00     0.00   \n",
       "2  0.000000  0.00  0.0   0.25   0.25  0.00  0.25  0.00  0.0    0.25     0.00   \n",
       "3  0.000000  0.00  0.0   0.00   0.00  0.25  0.00  0.00  0.0    0.00     0.25   \n",
       "4  0.000000  0.00  0.2   0.00   0.00  0.00  0.00  0.00  0.2    0.00     0.00   \n",
       "\n",
       "   mang  mangeon     matin   oiseaux  orang  pomm   sur  tapi  une  \n",
       "0   0.0     0.00  0.000000  0.000000    0.0  0.00  0.25  0.25  0.0  \n",
       "1   0.0     0.00  0.333333  0.333333    0.0  0.00  0.00  0.00  0.0  \n",
       "2   0.0     0.00  0.000000  0.000000    0.0  0.00  0.00  0.00  0.0  \n",
       "3   0.0     0.25  0.000000  0.000000    0.0  0.25  0.00  0.00  0.0  \n",
       "4   0.2     0.00  0.000000  0.000000    0.2  0.00  0.00  0.00  0.2  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83ac30",
   "metadata": {},
   "source": [
    "4. Appliquer la métrique similarité de cosinus entre les tokens « chat et chien », puis après « pomme et orange » sur les deux représentations vectorielles. Conclure ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e5ac539e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarité entre 'chat' et 'chien' : 0.0\n",
      "Similarité entre 'pomme' et 'orange' : 0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Définissez les mots pour lesquels vous souhaitez calculer la similarité\n",
    "mots_1 = [\"chat\", \"chien\"]\n",
    "mots_2 = [\"pomme\", \"orange\"]\n",
    "\n",
    "# Transformez les mots en vecteurs TF-IDF\n",
    "vecteurs_1 = tfidf_vectorizer.transform([\" \".join(mots_1)])\n",
    "vecteurs_2 = tfidf_vectorizer.transform([\" \".join(mots_2)])\n",
    "\n",
    "# Calculez la similarité cosinus entre les deux paires de mots\n",
    "similarite_1 = cosine_similarity(vecteurs_1, vecteurs_2)\n",
    "\n",
    "print(f\"Similarité entre 'chat' et 'chien' : {similarite_1[0][0]}\")\n",
    "print(f\"Similarité entre 'pomme' et 'orange' : {similarite_1[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ceca1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
