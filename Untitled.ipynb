{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5a844e",
   "metadata": {},
   "source": [
    "# Atelier1 : Représentation vectorielle de Texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b3162",
   "metadata": {},
   "source": [
    "### Partie 1 : Pré-traitement de texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb31bfb",
   "metadata": {},
   "source": [
    "1. Créer un corpus qui contient les textes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cf324488",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfdb32d",
   "metadata": {},
   "source": [
    "2. Convertir le corpus en type DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ce708faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte\n",
      "0        Le chat dort sur le tapis.\n",
      "1    Les Oiseaux Chantent Le Matin.\n",
      "2    Le chien court dans le jardin.\n",
      "3  Mangeons des pommes délicieuses.\n",
      "4      Je mange une orange fraîche.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'texte': corpus})\n",
    "\n",
    "# Affichez le DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fae0b50",
   "metadata": {},
   "source": [
    "3. Quelles sont les différentes approches pour prétraiter un corpus de textes? Décrire les.\n",
    "C’est quoi le rôle de chaque approche ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c3771d",
   "metadata": {},
   "source": [
    "## Tokenization (Tokenisation) :\n",
    "\n",
    "* Rôle :\n",
    "La tokenization consiste à diviser un texte en unités discrètes, généralement en mots ou en tokens. Cela permet de segmenter le texte en éléments distincts pour une analyse plus approfondie. Les tokens sont essentiels pour effectuer des opérations telles que la comptage de mots, l'analyse grammaticale, et la recherche d'entités nommées.\n",
    "\n",
    "## Suppression de la ponctuation :\n",
    "\n",
    "* Rôle : La suppression de la ponctuation implique de retirer les caractères de ponctuation du texte, tels que les points, les virgules, les guillemets, etc. Cela permet de nettoyer le texte des éléments non alphabétiques qui n'apportent généralement pas d'informations significatives pour de nombreuses tâches d'analyse de texte.\n",
    "\n",
    "## Mise en minuscules :\n",
    "\n",
    "* Rôle :\n",
    "La mise en minuscules consiste à convertir l'ensemble du texte en lettres minuscules. Cela permet de normaliser le texte et d'éviter les variations dues à la casse. Par exemple, \"Chat\" et \"chat\" seront considérés comme identiques après mise en minuscules.\n",
    "\n",
    "## Suppression des stopwords :\n",
    "\n",
    "* Rôle : \n",
    "Les stopwords sont des mots courants tels que \"le\", \"et\", \"de\" qui n'apportent généralement pas d'informations significatives dans l'analyse de texte. Les supprimer du texte permet de réduire le bruit et de se concentrer sur les mots clés importants.\n",
    "\n",
    "## Lemmatization (Lemmatisation) :\n",
    "\n",
    "* Rôle :\n",
    "La lemmatisation vise à réduire les mots à leur forme de base, appelée lemme. Par exemple, \"mangeons\" serait réduit à \"manger\". Cela aide à regrouper différentes formes d'un mot pour une analyse plus cohérente.\n",
    "Stemming (Racinisation) :\n",
    "\n",
    "## Stemming (Racinisation) :\n",
    "\n",
    "* Rôle :\n",
    "Le stemming réduit les mots à leur racine ou à leur forme tronquée, même si la forme résultante n'est pas toujours un mot réel. Par exemple, \"mangeons\" pourrait être réduit à \"mang\". Cette technique est plus agressive que la lemmatisation mais permet de regrouper des mots similaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a614a",
   "metadata": {},
   "source": [
    "4. Décrire le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee9e5db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)\n",
    "\n",
    "def code(texte):\n",
    "    text_s_pon = [c for c in texte if c not in string.punctuation]\n",
    "    return text_s_pon\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a04db5e",
   "metadata": {},
   "source": [
    "Ce code importe la bibliothèque string et imprime la ponctuation.\n",
    "Ensuite, il définit une fonction code(texte) qui prend un texte en entrée et renvoie le même\n",
    "texte, mais avec la ponctuation supprimée. \n",
    "La liste de caractères de ponctuation est obtenue à partir de string.punctuation. \n",
    "Par exemple, si vous appelez code(\"Le chat dort.\"), cela renverra \"Le chat dort\" en supprimant le point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d743446f",
   "metadata": {},
   "source": [
    "5. Ajouter une colonne dans l’objet corpus nommée « t_s_p » en utilisant la fonction de Q.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "036dff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis\n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin\n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin\n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses\n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# Créez un DataFrame à partir du corpus\n",
    "corpus = pd.DataFrame({'texte': [\n",
    "    \"Le chat dort sur le tapis.\",\n",
    "    \"Les Oiseaux Chantent Le Matin.\",\n",
    "    \"Le chien court dans le jardin.\",\n",
    "    \"Mangeons des pommes délicieuses.\",\n",
    "    \"Je mange une orange fraîche.\"\n",
    "]})\n",
    "\n",
    "# Fonction de suppression de la ponctuation\n",
    "def remove_punctuation(text):\n",
    "    # Utilisez la fonction translate pour supprimer la ponctuation\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    text_no_punct = text.translate(translator)\n",
    "    return text_no_punct\n",
    "\n",
    "# Appliquez la fonction de suppression de la ponctuation à la colonne \"texte\"\n",
    "corpus['t_s_p'] = corpus['texte'].apply(remove_punctuation)\n",
    "\n",
    "# Affichez le DataFrame avec la nouvelle colonne\n",
    "print(corpus)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca9c6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f2b3e59",
   "metadata": {},
   "source": [
    "6. Ecrire une fonction pour tokenizer le corpus de colonne \"t_s_p\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dd2f2a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              texte                            t_s_p  \\\n",
      "0        Le chat dort sur le tapis.        Le chat dort sur le tapis   \n",
      "1    Les Oiseaux Chantent Le Matin.    Les Oiseaux Chantent Le Matin   \n",
      "2    Le chien court dans le jardin.    Le chien court dans le jardin   \n",
      "3  Mangeons des pommes délicieuses.  Mangeons des pommes délicieuses   \n",
      "4      Je mange une orange fraîche.      Je mange une orange fraîche   \n",
      "\n",
      "                                 tokens  \n",
      "0      [Le, chat, dort, sur, le, tapis]  \n",
      "1   [Les, Oiseaux, Chantent, Le, Matin]  \n",
      "2  [Le, chien, court, dans, le, jardin]  \n",
      "3  [Mangeons, des, pommes, délicieuses]  \n",
      "4     [Je, mange, une, orange, fraîche]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# Fonction pour la tokenization\n",
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Appliquez la fonction de tokenization à la colonne \"t_s_p\"\n",
    "corpus['tokens'] = corpus['t_s_p'].apply(tokenize_text)\n",
    "\n",
    "# Affichez le DataFrame avec la nouvelle colonne \"tokens\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c304d0b",
   "metadata": {},
   "source": [
    "7. Ecrire une fonction qui élimine les stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bed9145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                [chat, dort, tapis]\n",
      "1         [Oiseaux, Chantent, Matin]\n",
      "2             [chien, court, jardin]\n",
      "3    [Mangeons, pommes, délicieuses]\n",
      "4           [mange, orange, fraîche]\n",
      "Name: no_stopwords, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé et téléchargez les stopwords si ce n'est pas déjà fait\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Fonction pour éliminer les stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    # Utilisez les stopwords de la langue française\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in french_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Exemple d'utilisation de la fonction\n",
    "corpus['no_stopwords'] = corpus['tokens'].apply(remove_stopwords)\n",
    "print(corpus['no_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90713671",
   "metadata": {},
   "source": [
    "8. Appliquer la lemmatisation et stremming sur le corpus sans stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e019740b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              t_s_p                                   tokens  \\\n",
      "0        Le chat dort sur le tapis.      [Le, chat, dort, sur, le, tapis, .]   \n",
      "1    Les Oiseaux Chantent Le Matin.   [Les, Oiseaux, Chantent, Le, Matin, .]   \n",
      "2    Le chien court dans le jardin.  [Le, chien, court, dans, le, jardin, .]   \n",
      "3  Mangeons des pommes délicieuses.  [Mangeons, des, pommes, délicieuses, .]   \n",
      "4      Je mange une orange fraîche.     [Je, mange, une, orange, fraîche, .]   \n",
      "\n",
      "                         no_stopwords                          lemmatized  \\\n",
      "0              [chat, dort, tapis, .]              [chat, dort, tapis, .]   \n",
      "1       [Oiseaux, Chantent, Matin, .]       [Oiseaux, Chantent, Matin, .]   \n",
      "2           [chien, court, jardin, .]           [chien, court, jardin, .]   \n",
      "3  [Mangeons, pommes, délicieuses, .]  [Mangeons, pommes, délicieuses, .]   \n",
      "4         [mange, orange, fraîche, .]         [mange, orange, fraîche, .]   \n",
      "\n",
      "                        stemmed  \n",
      "0        [chat, dort, tapis, .]  \n",
      "1  [oiseau, chantent, matin, .]  \n",
      "2     [chien, court, jardin, .]  \n",
      "3    [mangeon, pomm, délici, .]  \n",
      "4      [mang, orang, fraîch, .]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hisoka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Assurez-vous que NLTK est correctement installé et téléchargez les données nécessaires\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Fonction pour éliminer les stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    french_stopwords = set(stopwords.words('french'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in french_stopwords]\n",
    "    return filtered_tokens\n",
    "\n",
    "# Fonction pour lemmatiser les tokens\n",
    "def lemmatize(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return lemmatized\n",
    "\n",
    "# Fonction pour appliquer le stemming aux tokens\n",
    "def stem(tokens):\n",
    "    stemmer = SnowballStemmer('french')\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    return stemmed\n",
    "\n",
    "# Appliquez la suppression des stopwords à la colonne \"t_s_p\"\n",
    "corpus['no_stopwords'] = corpus['t_s_p'].apply(nltk.word_tokenize).apply(remove_stopwords)\n",
    "\n",
    "# Appliquez la lemmatisation aux tokens sans stopwords\n",
    "corpus['lemmatized'] = corpus['no_stopwords'].apply(lemmatize)\n",
    "\n",
    "# Appliquez le stemming aux tokens lemmatisés\n",
    "corpus['stemmed'] = corpus['lemmatized'].apply(stem)\n",
    "\n",
    "# Affichez le DataFrame avec les nouvelles colonnes\n",
    "print(corpus)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d594ac3",
   "metadata": {},
   "source": [
    "# Partie 2 : CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6958e546",
   "metadata": {},
   "source": [
    "\n",
    "9. Initialiser et ajuster le CountVectorizer à votre corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e683968",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Créez une instance de CountVectorizer\n",
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "# Ajustez le CountVectorizer sur votre corpus\n",
    "X = count_vectorizer.fit_transform(corpus['t_s_p'])\n",
    "\n",
    "# X est la matrice de comptage des tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8a023d",
   "metadata": {},
   "source": [
    "10. Transformer le corpus en une matrice de comptage de tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7cbb0fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 11)\t2\n",
      "  (0, 19)\t1\n",
      "  (0, 20)\t1\n"
     ]
    }
   ],
   "source": [
    "# Obtenez la première phrase du corpus\n",
    "texte = corpus['t_s_p'][0]\n",
    "\n",
    "# Transformez la phrase en un vecteur binaire en utilisant le CountVectorizer\n",
    "vector_binaire = count_vectorizer.transform([texte])\n",
    "\n",
    "# Affichez le vecteur binaire\n",
    "print(vector_binaire)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c968d65f",
   "metadata": {},
   "source": [
    "11. Explorer la matrice résultante pour comprendre comment les tokens sont représentés envecteurs binaires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c0402719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token : chat, Comptage : 1\n",
      "Token : dort, Comptage : 1\n",
      "Token : le, Comptage : 2\n",
      "Token : sur, Comptage : 1\n",
      "Token : tapis, Comptage : 1\n"
     ]
    }
   ],
   "source": [
    "# Obtenez les noms des tokens (mots)\n",
    "noms_tokens = count_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Obtenez les comptages de tokens pour le premier document (phrase)\n",
    "comptages_tokens_premier_document = X[0].toarray()\n",
    "\n",
    "# Affichez les noms des tokens et les comptages\n",
    "for nom_token, comptage in zip(noms_tokens, comptages_tokens_premier_document[0]):\n",
    "    if comptage > 0:\n",
    "        print(f\"Token : {nom_token}, Comptage : {comptage}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
